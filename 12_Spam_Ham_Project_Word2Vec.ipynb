{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b460fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy, gensim\n",
    "from gensim.models import Word2Vec, keyedvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a212b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(\n",
    "    r\"C:\\Users\\avira\\OneDrive\\Desktop\\UDEMY\\NLP & Deep Learning\\NLP\\GoogleNews-vectors-negative300.bin\",\n",
    "    binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73474591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1 attempted...\n",
      "Shape: (5574, 2)\n",
      "First few rows:\n",
      "                                               label  message\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...      NaN\n",
      "1                 ham\\tOk lar... Joking wif u oni...      NaN\n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...      NaN\n",
      "3  ham\\tU dun say so early hor... U c already the...      NaN\n",
      "4  ham\\tNah I don't think he goes to usf, he live...      NaN\n",
      "Tab separation failed - labels still contain tab characters\n",
      "Method 1 issue detected: Tab separation didn't work properly\n",
      "Method 2 (manual parsing) successful!\n",
      "Shape: (5574, 2)\n",
      "Sample data:\n",
      "  0: '\"ham' - 'Go until jurong point, crazy.. Available only in b...'\n",
      "  1: '\"ham' - 'Ok lar... Joking wif u oni...\"...'\n",
      "  2: '\"spam' - 'Free entry in 2 a wkly comp to win FA Cup final tk...'\n",
      "\n",
      "==================================================\n",
      "DATA VALIDATION AND CLEANUP:\n",
      "Dataset shape: (5574, 2)\n",
      "Any null values? 0\n",
      "Unique labels: ['\"ham' '\"spam']\n",
      "Final shape after cleanup: (5574, 2)\n",
      "Label distribution:\n",
      "label\n",
      "\"ham     4827\n",
      "\"spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 3 examples:\n",
      "  Row 0: [\"ham] Go until jurong point, crazy.. Available only in bugis n gre...\n",
      "  Row 1: [\"ham] Ok lar... Joking wif u oni...\"...\n",
      "  Row 2: [\"spam] Free entry in 2 a wkly comp to win FA Cup final tkts 21st Ma...\n",
      "\n",
      "Dataset is ready to use!\n",
      "Access with:\n",
      "  - messages['label'] for spam/ham labels\n",
      "  - messages['message'] for SMS text\n",
      "\n",
      "==================================================\n",
      "FIXING THE COLUMN SPLITTING:\n",
      "Data appears to be properly formatted already.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Method 1: More robust loading with error handling\n",
    "try:\n",
    "    # Try with explicit encoding and error handling\n",
    "    messages = pd.read_csv(r'C:\\Users\\avira\\OneDrive\\Desktop\\UDEMY\\NLP & Deep Learning\\NLP\\SMSSpamCollection.csv', \n",
    "                          sep='\\t', \n",
    "                          names=['label', 'message'], \n",
    "                          encoding='utf-8',\n",
    "                          engine='python',\n",
    "                          on_bad_lines='skip')\n",
    "    print(\"Method 1 attempted...\")\n",
    "    print(f\"Shape: {messages.shape}\")\n",
    "    print(\"First few rows:\")\n",
    "    print(messages.head())\n",
    "    \n",
    "    # Check if tab separation actually worked\n",
    "    if messages.shape[1] == 2:\n",
    "        # Check if the first column still contains tabs (meaning separation failed)\n",
    "        sample_label = str(messages.iloc[0]['label']) if len(messages) > 0 else \"\"\n",
    "        if '\\t' in sample_label:\n",
    "            print(\"Tab separation failed - labels still contain tab characters\")\n",
    "            raise Exception(\"Tab separation didn't work properly\")\n",
    "        else:\n",
    "            print(\"Method 1 successful - tab separation worked!\")\n",
    "    else:\n",
    "        raise Exception(\"Wrong number of columns\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Method 1 issue detected: {e}\")\n",
    "    \n",
    "    # Method 2: Manual splitting approach (most reliable)\n",
    "    try:\n",
    "        # Read the file as a single column first\n",
    "        with open(r'C:\\Users\\avira\\OneDrive\\Desktop\\UDEMY\\NLP & Deep Learning\\NLP\\SMSSpamCollection.csv', 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Parse each line manually\n",
    "        labels = []\n",
    "        messages_text = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                # Split on the first tab only\n",
    "                parts = line.split('\\t', 1)\n",
    "                if len(parts) == 2:\n",
    "                    labels.append(parts[0])\n",
    "                    messages_text.append(parts[1])\n",
    "        \n",
    "        # Create dataframe\n",
    "        messages = pd.DataFrame({\n",
    "            'label': labels,\n",
    "            'message': messages_text\n",
    "        })\n",
    "        \n",
    "        print(\"Method 2 (manual parsing) successful!\")\n",
    "        print(f\"Shape: {messages.shape}\")\n",
    "        print(\"Sample data:\")\n",
    "        for i in range(min(3, len(messages))):\n",
    "            print(f\"  {i}: '{messages.iloc[i]['label']}' - '{messages.iloc[i]['message'][:50]}...'\")\n",
    "            \n",
    "    except Exception as e2:\n",
    "        print(f\"Method 2 failed: {e2}\")\n",
    "        \n",
    "        # Method 3: Let pandas auto-detect\n",
    "        try:\n",
    "            messages = pd.read_csv(r'C:\\Users\\avira\\OneDrive\\Desktop\\UDEMY\\NLP & Deep Learning\\NLP\\SMSSpamCollection.csv', \n",
    "                                  sep='\\t', \n",
    "                                  header=None,\n",
    "                                  encoding='utf-8')\n",
    "            # Manually assign column names\n",
    "            if messages.shape[1] >= 2:\n",
    "                messages.columns = ['label', 'message'] + [f'extra_{i}' for i in range(2, messages.shape[1])]\n",
    "                # Keep only first two columns if there are extras\n",
    "                messages = messages[['label', 'message']]\n",
    "            print(\"Method 3 (auto-detect) successful!\")\n",
    "            print(f\"Shape: {messages.shape}\")\n",
    "            print(messages.head())\n",
    "            \n",
    "        except Exception as e3:\n",
    "            print(f\"All methods failed. Last error: {e3}\")\n",
    "\n",
    "# Clean up and validate the data\n",
    "if 'messages' in locals() and len(messages) > 0:\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"DATA VALIDATION AND CLEANUP:\")\n",
    "    \n",
    "    # Check for any remaining issues\n",
    "    print(f\"Dataset shape: {messages.shape}\")\n",
    "    print(f\"Any null values? {messages.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Validate labels\n",
    "    unique_labels = messages['label'].unique()\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "    \n",
    "    # Clean up any whitespace\n",
    "    messages['label'] = messages['label'].str.strip()\n",
    "    messages['message'] = messages['message'].str.strip()\n",
    "    \n",
    "    # Remove any empty rows\n",
    "    messages = messages[(messages['label'] != '') & (messages['message'] != '')]\n",
    "    \n",
    "    print(f\"Final shape after cleanup: {messages.shape}\")\n",
    "    print(f\"Label distribution:\")\n",
    "    print(messages['label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nFirst 3 examples:\")\n",
    "    for i in range(min(3, len(messages))):\n",
    "        label = messages.iloc[i]['label']\n",
    "        message = messages.iloc[i]['message']\n",
    "        print(f\"  Row {i}: [{label}] {message[:60]}...\")\n",
    "    \n",
    "    print(f\"\\nDataset is ready to use!\")\n",
    "    print(\"Access with:\")\n",
    "    print(\"  - messages['label'] for spam/ham labels\") \n",
    "    print(\"  - messages['message'] for SMS text\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid data loaded. Please check your file path and format.\")\n",
    "\n",
    "# Fix the column splitting issue\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FIXING THE COLUMN SPLITTING:\")\n",
    "\n",
    "if 'messages' in locals() and messages.shape[1] == 1:\n",
    "    print(\"Data loaded but columns not split properly. Fixing...\")\n",
    "    \n",
    "    # Get the single column that contains both label and message\n",
    "    combined_column = messages.iloc[:, 0]\n",
    "    \n",
    "    # Split on the first tab character\n",
    "    split_data = combined_column.str.split('\\t', n=1, expand=True)\n",
    "    \n",
    "    # Create new dataframe with proper columns\n",
    "    messages_fixed = pd.DataFrame({\n",
    "        'label': split_data[0],\n",
    "        'message': split_data[1]\n",
    "    })\n",
    "    \n",
    "    # Remove any rows where splitting failed\n",
    "    messages_fixed = messages_fixed.dropna()\n",
    "    \n",
    "    print(f\"Fixed dataset shape: {messages_fixed.shape}\")\n",
    "    print(f\"Label distribution:\")\n",
    "    print(messages_fixed['label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nFirst 5 rows after fixing:\")\n",
    "    for i in range(min(5, len(messages_fixed))):\n",
    "        label = messages_fixed.iloc[i]['label']\n",
    "        message = messages_fixed.iloc[i]['message'][:50]\n",
    "        print(f\"{i}: {label} - {message}...\")\n",
    "    \n",
    "    # Replace the original messages dataframe\n",
    "    messages = messages_fixed\n",
    "    print(f\"\\nFinal dataset ready to use!\")\n",
    "    print(f\"Access with: messages['label'] and messages['message']\")\n",
    "\n",
    "else:\n",
    "    print(\"Data appears to be properly formatted already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c87800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up quotation marks...\n",
      "Cleanup complete!\n",
      "\n",
      "Cleaned data preview:\n",
      "Shape: (5574, 2)\n",
      "Row 0: [ham] Go until jurong point, crazy.. Available only in bugis n gre...\n",
      "Row 1: [ham] Ok lar... Joking wif u oni......\n",
      "Row 2: [spam] Free entry in 2 a wkly comp to win FA Cup final tkts 21st Ma...\n",
      "Row 3: [ham] U dun say so early hor... U c already then say......\n",
      "Row 4: [ham] Nah I don't think he goes to usf, he lives around here thoug...\n",
      "\n",
      "Label distribution after cleanup:\n",
      "label\n",
      "ham     4827\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique labels: ['ham' 'spam']\n",
      "\n",
      "Quotes remaining in labels: 0\n",
      "Quotes remaining in messages: 0\n",
      "\n",
      "✅ Dataset is now clean and ready for NLP analysis!\n"
     ]
    }
   ],
   "source": [
    "# Clean up the quotation marks from labels and messages\n",
    "print(\"Cleaning up quotation marks...\")\n",
    "\n",
    "# Remove quotes from labels (ham/spam)\n",
    "messages['label'] = messages['label'].str.replace('\"', '', regex=False)\n",
    "\n",
    "# Remove quotes from the beginning and end of messages\n",
    "messages['message'] = messages['message'].str.replace('\"', '', regex=False)\n",
    "\n",
    "print(\"Cleanup complete!\")\n",
    "print(f\"\\nCleaned data preview:\")\n",
    "print(f\"Shape: {messages.shape}\")\n",
    "\n",
    "# Show first few rows to verify cleanup\n",
    "for i in range(5):\n",
    "    label = messages.iloc[i]['label']\n",
    "    message = messages.iloc[i]['message'][:60]\n",
    "    print(f\"Row {i}: [{label}] {message}...\")\n",
    "\n",
    "print(f\"\\nLabel distribution after cleanup:\")\n",
    "print(messages['label'].value_counts())\n",
    "\n",
    "print(f\"\\nUnique labels: {messages['label'].unique()}\")\n",
    "\n",
    "# Verify no quotes remain\n",
    "has_quotes_labels = messages['label'].str.contains('\"', na=False).sum()\n",
    "has_quotes_messages = messages['message'].str.contains('\"', na=False).sum()\n",
    "\n",
    "print(f\"\\nQuotes remaining in labels: {has_quotes_labels}\")\n",
    "print(f\"Quotes remaining in messages: {has_quotes_messages}\")\n",
    "\n",
    "print(f\"\\n✅ Dataset is now clean and ready for NLP analysis!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3178d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5569  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5570   ham               Will ü b going to esplanade fr home?\n",
       "5571   ham  Pity, * was in mood for that. So...any other s...\n",
       "5572   ham  The guy did some bitching but I acted like i'd...\n",
       "5573   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5574 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76fd2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328439d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ad2ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(message)):\n",
    "  review = re.sub('[^a-zA-Z]',' ', messages['message'][i])\n",
    "  review = review.lower()\n",
    "  review = review.split()\n",
    "  \n",
    "  review = [lemmatizer.lemmatize(word) for word in review]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d8b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df563d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat',\n",
       " 'ok lar joking wif u oni',\n",
       " 'free entry in a wkly comp to win fa cup final tkts st may text fa to to receive entry question std txt rate t c s apply over s',\n",
       " 'u dun say so early hor u c already then say',\n",
       " 'nah i don t think he go to usf he life around here though',\n",
       " 'freemsg hey there darling it s been week s now and no word back i d like some fun you up for it still tb ok xxx std chgs to send to rcv',\n",
       " 'even my brother is not like to speak with me they treat me like aid patent',\n",
       " 'a per your request melle melle oru minnaminunginte nurungu vettam ha been set a your callertune for all caller press to copy your friend callertune',\n",
       " 'winner a a valued network customer you have been selected to receivea prize reward to claim call claim code kl valid hour only',\n",
       " 'had your mobile month or more u r entitled to update to the latest colour mobile with camera for free call the mobile update co free on',\n",
       " 'i m gonna be home soon and i don t want to talk about this stuff anymore tonight k i ve cried enough today',\n",
       " 'six chance to win cash from to pound txt csh and send to cost p day day tsandcs apply reply hl info',\n",
       " 'urgent you have won a week free membership in our prize jackpot txt the word claim to no t c www dbuk net lccltd pobox ldnw a rw',\n",
       " 'i ve been searching for the right word to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all time',\n",
       " 'i have a date on sunday with will',\n",
       " 'xxxmobilemovieclub to use your credit click the wap link in the next txt message or click here http wap xxxmobilemovieclub com n qjkgighjjgcbl',\n",
       " 'oh k i m watching here',\n",
       " 'eh u remember how spell his name yes i did he v naughty make until i v wet',\n",
       " 'fine if that s the way u feel that s the way it gota b',\n",
       " 'england v macedonia dont miss the goal team news txt ur national team to eg england to try wale scotland txt poboxox w wq',\n",
       " 'is that seriously how you spell his name',\n",
       " 'i m going to try for month ha ha only joking',\n",
       " 'so pay first lar then when is da stock comin',\n",
       " 'aft i finish my lunch then i go str down lor ard smth lor u finish ur lunch already',\n",
       " 'ffffffffff alright no way i can meet up with you sooner',\n",
       " 'just forced myself to eat a slice i m really not hungry tho this suck mark is getting worried he know i m sick when i turn down pizza lol',\n",
       " 'lol your always so convincing',\n",
       " 'did you catch the bus are you frying an egg did you make a tea are you eating your mom s left over dinner do you feel my love',\n",
       " 'i m back amp we re packing the car now i ll let you know if there s room',\n",
       " 'ahhh work i vaguely remember that what doe it feel like lol',\n",
       " 'wait that s still not all that clear were you not sure about me being sarcastic or that that s why x doesn t want to live with u',\n",
       " 'yeah he got in at and wa v apologetic n had fallen out and she wa actin like spoilt child and he got caught up in that till but we won t go there not doing too badly cheer you',\n",
       " 'k tell me anything about you',\n",
       " 'for fear of fainting with the of all that housework you just did quick have a cuppa',\n",
       " 'thanks for your subscription to ringtone uk your mobile will be charged month please confirm by replying yes or no if you reply no you will not be charged',\n",
       " 'yup ok i go home look at the timing then i msg again xuhui going to learn on nd may too but her lesson is at am',\n",
       " 'oops i ll let you know when my roommate s done',\n",
       " 'i see the letter b on my car',\n",
       " 'anything lor u decide',\n",
       " 'hello how s you and how did saturday go i wa just texting to see if you d decided to do anything tomo not that i m trying to invite myself or anything',\n",
       " 'pls go ahead with watt i just wanted to be sure do have a great weekend abiola',\n",
       " 'did i forget to tell you i want you i need you i crave you but most of all i love you my sweet arabian steed mmmmmm yummy',\n",
       " 'rodger burn msg we tried to call you re your reply to our sm for a free nokia mobile free camcorder please call now for delivery tomorrow',\n",
       " 'who are you seeing',\n",
       " 'great i hope you like your man well endowed i am lt gt inch',\n",
       " 'no call message missed call',\n",
       " 'didn t you get hep b immunisation in nigeria',\n",
       " 'fair enough anything going on',\n",
       " 'yeah hopefully if tyler can t do it i could maybe ask around a bit',\n",
       " 'u don t know how stubborn i am i didn t even want to go to the hospital i kept telling mark i m not a weak sucker hospital are for weak sucker',\n",
       " 'what you thinked about me first time you saw me in class',\n",
       " 'a gram usually run like lt gt a half eighth is smarter though and get you almost a whole second gram for lt gt',\n",
       " 'k fyi x ha a ride early tomorrow morning but he s crashing at our place tonight',\n",
       " 'wow i never realized that you were so embarassed by your accomodations i thought you liked it since i wa doing the best i could and you always seemed so happy about the cave i m sorry i didn t and don t have more to give i m sorry i offered i m sorry your room wa so embarassing',\n",
       " 'sm ac sptv the new jersey devil and the detroit red wing play ice hockey correct or incorrect end reply end sptv',\n",
       " 'do you know what mallika sherawat did yesterday find out now lt url gt',\n",
       " 'congrats year special cinema pas for is yours call now c suprman v matrix starwars etc all free bx ip we pm dont miss out',\n",
       " 'sorry i ll call later in meeting',\n",
       " 'tell where you reached',\n",
       " 'yes gauti and sehwag out of odi series']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f83f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17306353",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for sent in corpus:\n",
    "  sent_token = sent_tokenize(sent)\n",
    "  for sent in sent_token:\n",
    "    words.append(simple_preprocess(sent))\n",
    "    \n",
    "## Simple preprocess : convert a document unto a list of lower case tokens, ignoring tokens that are too short or too long\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "787de7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['go',\n",
       "  'until',\n",
       "  'jurong',\n",
       "  'point',\n",
       "  'crazy',\n",
       "  'available',\n",
       "  'only',\n",
       "  'in',\n",
       "  'bugis',\n",
       "  'great',\n",
       "  'world',\n",
       "  'la',\n",
       "  'buffet',\n",
       "  'cine',\n",
       "  'there',\n",
       "  'got',\n",
       "  'amore',\n",
       "  'wat'],\n",
       " ['ok', 'lar', 'joking', 'wif', 'oni'],\n",
       " ['free',\n",
       "  'entry',\n",
       "  'in',\n",
       "  'wkly',\n",
       "  'comp',\n",
       "  'to',\n",
       "  'win',\n",
       "  'fa',\n",
       "  'cup',\n",
       "  'final',\n",
       "  'tkts',\n",
       "  'st',\n",
       "  'may',\n",
       "  'text',\n",
       "  'fa',\n",
       "  'to',\n",
       "  'to',\n",
       "  'receive',\n",
       "  'entry',\n",
       "  'question',\n",
       "  'std',\n",
       "  'txt',\n",
       "  'rate',\n",
       "  'apply',\n",
       "  'over'],\n",
       " ['dun', 'say', 'so', 'early', 'hor', 'already', 'then', 'say'],\n",
       " ['nah',\n",
       "  'don',\n",
       "  'think',\n",
       "  'he',\n",
       "  'go',\n",
       "  'to',\n",
       "  'usf',\n",
       "  'he',\n",
       "  'life',\n",
       "  'around',\n",
       "  'here',\n",
       "  'though'],\n",
       " ['freemsg',\n",
       "  'hey',\n",
       "  'there',\n",
       "  'darling',\n",
       "  'it',\n",
       "  'been',\n",
       "  'week',\n",
       "  'now',\n",
       "  'and',\n",
       "  'no',\n",
       "  'word',\n",
       "  'back',\n",
       "  'like',\n",
       "  'some',\n",
       "  'fun',\n",
       "  'you',\n",
       "  'up',\n",
       "  'for',\n",
       "  'it',\n",
       "  'still',\n",
       "  'tb',\n",
       "  'ok',\n",
       "  'xxx',\n",
       "  'std',\n",
       "  'chgs',\n",
       "  'to',\n",
       "  'send',\n",
       "  'to',\n",
       "  'rcv'],\n",
       " ['even',\n",
       "  'my',\n",
       "  'brother',\n",
       "  'is',\n",
       "  'not',\n",
       "  'like',\n",
       "  'to',\n",
       "  'speak',\n",
       "  'with',\n",
       "  'me',\n",
       "  'they',\n",
       "  'treat',\n",
       "  'me',\n",
       "  'like',\n",
       "  'aid',\n",
       "  'patent'],\n",
       " ['per',\n",
       "  'your',\n",
       "  'request',\n",
       "  'melle',\n",
       "  'melle',\n",
       "  'oru',\n",
       "  'minnaminunginte',\n",
       "  'nurungu',\n",
       "  'vettam',\n",
       "  'ha',\n",
       "  'been',\n",
       "  'set',\n",
       "  'your',\n",
       "  'callertune',\n",
       "  'for',\n",
       "  'all',\n",
       "  'caller',\n",
       "  'press',\n",
       "  'to',\n",
       "  'copy',\n",
       "  'your',\n",
       "  'friend',\n",
       "  'callertune'],\n",
       " ['winner',\n",
       "  'valued',\n",
       "  'network',\n",
       "  'customer',\n",
       "  'you',\n",
       "  'have',\n",
       "  'been',\n",
       "  'selected',\n",
       "  'to',\n",
       "  'receivea',\n",
       "  'prize',\n",
       "  'reward',\n",
       "  'to',\n",
       "  'claim',\n",
       "  'call',\n",
       "  'claim',\n",
       "  'code',\n",
       "  'kl',\n",
       "  'valid',\n",
       "  'hour',\n",
       "  'only'],\n",
       " ['had',\n",
       "  'your',\n",
       "  'mobile',\n",
       "  'month',\n",
       "  'or',\n",
       "  'more',\n",
       "  'entitled',\n",
       "  'to',\n",
       "  'update',\n",
       "  'to',\n",
       "  'the',\n",
       "  'latest',\n",
       "  'colour',\n",
       "  'mobile',\n",
       "  'with',\n",
       "  'camera',\n",
       "  'for',\n",
       "  'free',\n",
       "  'call',\n",
       "  'the',\n",
       "  'mobile',\n",
       "  'update',\n",
       "  'co',\n",
       "  'free',\n",
       "  'on'],\n",
       " ['gonna',\n",
       "  'be',\n",
       "  'home',\n",
       "  'soon',\n",
       "  'and',\n",
       "  'don',\n",
       "  'want',\n",
       "  'to',\n",
       "  'talk',\n",
       "  'about',\n",
       "  'this',\n",
       "  'stuff',\n",
       "  'anymore',\n",
       "  'tonight',\n",
       "  've',\n",
       "  'cried',\n",
       "  'enough',\n",
       "  'today'],\n",
       " ['six',\n",
       "  'chance',\n",
       "  'to',\n",
       "  'win',\n",
       "  'cash',\n",
       "  'from',\n",
       "  'to',\n",
       "  'pound',\n",
       "  'txt',\n",
       "  'csh',\n",
       "  'and',\n",
       "  'send',\n",
       "  'to',\n",
       "  'cost',\n",
       "  'day',\n",
       "  'day',\n",
       "  'tsandcs',\n",
       "  'apply',\n",
       "  'reply',\n",
       "  'hl',\n",
       "  'info'],\n",
       " ['urgent',\n",
       "  'you',\n",
       "  'have',\n",
       "  'won',\n",
       "  'week',\n",
       "  'free',\n",
       "  'membership',\n",
       "  'in',\n",
       "  'our',\n",
       "  'prize',\n",
       "  'jackpot',\n",
       "  'txt',\n",
       "  'the',\n",
       "  'word',\n",
       "  'claim',\n",
       "  'to',\n",
       "  'no',\n",
       "  'www',\n",
       "  'dbuk',\n",
       "  'net',\n",
       "  'lccltd',\n",
       "  'pobox',\n",
       "  'ldnw',\n",
       "  'rw'],\n",
       " ['ve',\n",
       "  'been',\n",
       "  'searching',\n",
       "  'for',\n",
       "  'the',\n",
       "  'right',\n",
       "  'word',\n",
       "  'to',\n",
       "  'thank',\n",
       "  'you',\n",
       "  'for',\n",
       "  'this',\n",
       "  'breather',\n",
       "  'promise',\n",
       "  'wont',\n",
       "  'take',\n",
       "  'your',\n",
       "  'help',\n",
       "  'for',\n",
       "  'granted',\n",
       "  'and',\n",
       "  'will',\n",
       "  'fulfil',\n",
       "  'my',\n",
       "  'promise',\n",
       "  'you',\n",
       "  'have',\n",
       "  'been',\n",
       "  'wonderful',\n",
       "  'and',\n",
       "  'blessing',\n",
       "  'at',\n",
       "  'all',\n",
       "  'time'],\n",
       " ['have', 'date', 'on', 'sunday', 'with', 'will'],\n",
       " ['to',\n",
       "  'use',\n",
       "  'your',\n",
       "  'credit',\n",
       "  'click',\n",
       "  'the',\n",
       "  'wap',\n",
       "  'link',\n",
       "  'in',\n",
       "  'the',\n",
       "  'next',\n",
       "  'txt',\n",
       "  'message',\n",
       "  'or',\n",
       "  'click',\n",
       "  'here',\n",
       "  'http',\n",
       "  'wap',\n",
       "  'com',\n",
       "  'qjkgighjjgcbl'],\n",
       " ['oh', 'watching', 'here'],\n",
       " ['eh',\n",
       "  'remember',\n",
       "  'how',\n",
       "  'spell',\n",
       "  'his',\n",
       "  'name',\n",
       "  'yes',\n",
       "  'did',\n",
       "  'he',\n",
       "  'naughty',\n",
       "  'make',\n",
       "  'until',\n",
       "  'wet'],\n",
       " ['fine',\n",
       "  'if',\n",
       "  'that',\n",
       "  'the',\n",
       "  'way',\n",
       "  'feel',\n",
       "  'that',\n",
       "  'the',\n",
       "  'way',\n",
       "  'it',\n",
       "  'gota'],\n",
       " ['england',\n",
       "  'macedonia',\n",
       "  'dont',\n",
       "  'miss',\n",
       "  'the',\n",
       "  'goal',\n",
       "  'team',\n",
       "  'news',\n",
       "  'txt',\n",
       "  'ur',\n",
       "  'national',\n",
       "  'team',\n",
       "  'to',\n",
       "  'eg',\n",
       "  'england',\n",
       "  'to',\n",
       "  'try',\n",
       "  'wale',\n",
       "  'scotland',\n",
       "  'txt',\n",
       "  'poboxox',\n",
       "  'wq'],\n",
       " ['is', 'that', 'seriously', 'how', 'you', 'spell', 'his', 'name'],\n",
       " ['going', 'to', 'try', 'for', 'month', 'ha', 'ha', 'only', 'joking'],\n",
       " ['so', 'pay', 'first', 'lar', 'then', 'when', 'is', 'da', 'stock', 'comin'],\n",
       " ['aft',\n",
       "  'finish',\n",
       "  'my',\n",
       "  'lunch',\n",
       "  'then',\n",
       "  'go',\n",
       "  'str',\n",
       "  'down',\n",
       "  'lor',\n",
       "  'ard',\n",
       "  'smth',\n",
       "  'lor',\n",
       "  'finish',\n",
       "  'ur',\n",
       "  'lunch',\n",
       "  'already'],\n",
       " ['ffffffffff',\n",
       "  'alright',\n",
       "  'no',\n",
       "  'way',\n",
       "  'can',\n",
       "  'meet',\n",
       "  'up',\n",
       "  'with',\n",
       "  'you',\n",
       "  'sooner'],\n",
       " ['just',\n",
       "  'forced',\n",
       "  'myself',\n",
       "  'to',\n",
       "  'eat',\n",
       "  'slice',\n",
       "  'really',\n",
       "  'not',\n",
       "  'hungry',\n",
       "  'tho',\n",
       "  'this',\n",
       "  'suck',\n",
       "  'mark',\n",
       "  'is',\n",
       "  'getting',\n",
       "  'worried',\n",
       "  'he',\n",
       "  'know',\n",
       "  'sick',\n",
       "  'when',\n",
       "  'turn',\n",
       "  'down',\n",
       "  'pizza',\n",
       "  'lol'],\n",
       " ['lol', 'your', 'always', 'so', 'convincing'],\n",
       " ['did',\n",
       "  'you',\n",
       "  'catch',\n",
       "  'the',\n",
       "  'bus',\n",
       "  'are',\n",
       "  'you',\n",
       "  'frying',\n",
       "  'an',\n",
       "  'egg',\n",
       "  'did',\n",
       "  'you',\n",
       "  'make',\n",
       "  'tea',\n",
       "  'are',\n",
       "  'you',\n",
       "  'eating',\n",
       "  'your',\n",
       "  'mom',\n",
       "  'left',\n",
       "  'over',\n",
       "  'dinner',\n",
       "  'do',\n",
       "  'you',\n",
       "  'feel',\n",
       "  'my',\n",
       "  'love'],\n",
       " ['back',\n",
       "  'amp',\n",
       "  'we',\n",
       "  're',\n",
       "  'packing',\n",
       "  'the',\n",
       "  'car',\n",
       "  'now',\n",
       "  'll',\n",
       "  'let',\n",
       "  'you',\n",
       "  'know',\n",
       "  'if',\n",
       "  'there',\n",
       "  'room'],\n",
       " ['ahhh',\n",
       "  'work',\n",
       "  'vaguely',\n",
       "  'remember',\n",
       "  'that',\n",
       "  'what',\n",
       "  'doe',\n",
       "  'it',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'lol'],\n",
       " ['wait',\n",
       "  'that',\n",
       "  'still',\n",
       "  'not',\n",
       "  'all',\n",
       "  'that',\n",
       "  'clear',\n",
       "  'were',\n",
       "  'you',\n",
       "  'not',\n",
       "  'sure',\n",
       "  'about',\n",
       "  'me',\n",
       "  'being',\n",
       "  'sarcastic',\n",
       "  'or',\n",
       "  'that',\n",
       "  'that',\n",
       "  'why',\n",
       "  'doesn',\n",
       "  'want',\n",
       "  'to',\n",
       "  'live',\n",
       "  'with'],\n",
       " ['yeah',\n",
       "  'he',\n",
       "  'got',\n",
       "  'in',\n",
       "  'at',\n",
       "  'and',\n",
       "  'wa',\n",
       "  'apologetic',\n",
       "  'had',\n",
       "  'fallen',\n",
       "  'out',\n",
       "  'and',\n",
       "  'she',\n",
       "  'wa',\n",
       "  'actin',\n",
       "  'like',\n",
       "  'spoilt',\n",
       "  'child',\n",
       "  'and',\n",
       "  'he',\n",
       "  'got',\n",
       "  'caught',\n",
       "  'up',\n",
       "  'in',\n",
       "  'that',\n",
       "  'till',\n",
       "  'but',\n",
       "  'we',\n",
       "  'won',\n",
       "  'go',\n",
       "  'there',\n",
       "  'not',\n",
       "  'doing',\n",
       "  'too',\n",
       "  'badly',\n",
       "  'cheer',\n",
       "  'you'],\n",
       " ['tell', 'me', 'anything', 'about', 'you'],\n",
       " ['for',\n",
       "  'fear',\n",
       "  'of',\n",
       "  'fainting',\n",
       "  'with',\n",
       "  'the',\n",
       "  'of',\n",
       "  'all',\n",
       "  'that',\n",
       "  'housework',\n",
       "  'you',\n",
       "  'just',\n",
       "  'did',\n",
       "  'quick',\n",
       "  'have',\n",
       "  'cuppa'],\n",
       " ['thanks',\n",
       "  'for',\n",
       "  'your',\n",
       "  'subscription',\n",
       "  'to',\n",
       "  'ringtone',\n",
       "  'uk',\n",
       "  'your',\n",
       "  'mobile',\n",
       "  'will',\n",
       "  'be',\n",
       "  'charged',\n",
       "  'month',\n",
       "  'please',\n",
       "  'confirm',\n",
       "  'by',\n",
       "  'replying',\n",
       "  'yes',\n",
       "  'or',\n",
       "  'no',\n",
       "  'if',\n",
       "  'you',\n",
       "  'reply',\n",
       "  'no',\n",
       "  'you',\n",
       "  'will',\n",
       "  'not',\n",
       "  'be',\n",
       "  'charged'],\n",
       " ['yup',\n",
       "  'ok',\n",
       "  'go',\n",
       "  'home',\n",
       "  'look',\n",
       "  'at',\n",
       "  'the',\n",
       "  'timing',\n",
       "  'then',\n",
       "  'msg',\n",
       "  'again',\n",
       "  'xuhui',\n",
       "  'going',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'on',\n",
       "  'nd',\n",
       "  'may',\n",
       "  'too',\n",
       "  'but',\n",
       "  'her',\n",
       "  'lesson',\n",
       "  'is',\n",
       "  'at',\n",
       "  'am'],\n",
       " ['oops', 'll', 'let', 'you', 'know', 'when', 'my', 'roommate', 'done'],\n",
       " ['see', 'the', 'letter', 'on', 'my', 'car'],\n",
       " ['anything', 'lor', 'decide'],\n",
       " ['hello',\n",
       "  'how',\n",
       "  'you',\n",
       "  'and',\n",
       "  'how',\n",
       "  'did',\n",
       "  'saturday',\n",
       "  'go',\n",
       "  'wa',\n",
       "  'just',\n",
       "  'texting',\n",
       "  'to',\n",
       "  'see',\n",
       "  'if',\n",
       "  'you',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'do',\n",
       "  'anything',\n",
       "  'tomo',\n",
       "  'not',\n",
       "  'that',\n",
       "  'trying',\n",
       "  'to',\n",
       "  'invite',\n",
       "  'myself',\n",
       "  'or',\n",
       "  'anything'],\n",
       " ['pls',\n",
       "  'go',\n",
       "  'ahead',\n",
       "  'with',\n",
       "  'watt',\n",
       "  'just',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'be',\n",
       "  'sure',\n",
       "  'do',\n",
       "  'have',\n",
       "  'great',\n",
       "  'weekend',\n",
       "  'abiola'],\n",
       " ['did',\n",
       "  'forget',\n",
       "  'to',\n",
       "  'tell',\n",
       "  'you',\n",
       "  'want',\n",
       "  'you',\n",
       "  'need',\n",
       "  'you',\n",
       "  'crave',\n",
       "  'you',\n",
       "  'but',\n",
       "  'most',\n",
       "  'of',\n",
       "  'all',\n",
       "  'love',\n",
       "  'you',\n",
       "  'my',\n",
       "  'sweet',\n",
       "  'arabian',\n",
       "  'steed',\n",
       "  'mmmmmm',\n",
       "  'yummy'],\n",
       " ['rodger',\n",
       "  'burn',\n",
       "  'msg',\n",
       "  'we',\n",
       "  'tried',\n",
       "  'to',\n",
       "  'call',\n",
       "  'you',\n",
       "  're',\n",
       "  'your',\n",
       "  'reply',\n",
       "  'to',\n",
       "  'our',\n",
       "  'sm',\n",
       "  'for',\n",
       "  'free',\n",
       "  'nokia',\n",
       "  'mobile',\n",
       "  'free',\n",
       "  'camcorder',\n",
       "  'please',\n",
       "  'call',\n",
       "  'now',\n",
       "  'for',\n",
       "  'delivery',\n",
       "  'tomorrow'],\n",
       " ['who', 'are', 'you', 'seeing'],\n",
       " ['great',\n",
       "  'hope',\n",
       "  'you',\n",
       "  'like',\n",
       "  'your',\n",
       "  'man',\n",
       "  'well',\n",
       "  'endowed',\n",
       "  'am',\n",
       "  'lt',\n",
       "  'gt',\n",
       "  'inch'],\n",
       " ['no', 'call', 'message', 'missed', 'call'],\n",
       " ['didn', 'you', 'get', 'hep', 'immunisation', 'in', 'nigeria'],\n",
       " ['fair', 'enough', 'anything', 'going', 'on'],\n",
       " ['yeah',\n",
       "  'hopefully',\n",
       "  'if',\n",
       "  'tyler',\n",
       "  'can',\n",
       "  'do',\n",
       "  'it',\n",
       "  'could',\n",
       "  'maybe',\n",
       "  'ask',\n",
       "  'around',\n",
       "  'bit'],\n",
       " ['don',\n",
       "  'know',\n",
       "  'how',\n",
       "  'stubborn',\n",
       "  'am',\n",
       "  'didn',\n",
       "  'even',\n",
       "  'want',\n",
       "  'to',\n",
       "  'go',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hospital',\n",
       "  'kept',\n",
       "  'telling',\n",
       "  'mark',\n",
       "  'not',\n",
       "  'weak',\n",
       "  'sucker',\n",
       "  'hospital',\n",
       "  'are',\n",
       "  'for',\n",
       "  'weak',\n",
       "  'sucker'],\n",
       " ['what',\n",
       "  'you',\n",
       "  'thinked',\n",
       "  'about',\n",
       "  'me',\n",
       "  'first',\n",
       "  'time',\n",
       "  'you',\n",
       "  'saw',\n",
       "  'me',\n",
       "  'in',\n",
       "  'class'],\n",
       " ['gram',\n",
       "  'usually',\n",
       "  'run',\n",
       "  'like',\n",
       "  'lt',\n",
       "  'gt',\n",
       "  'half',\n",
       "  'eighth',\n",
       "  'is',\n",
       "  'smarter',\n",
       "  'though',\n",
       "  'and',\n",
       "  'get',\n",
       "  'you',\n",
       "  'almost',\n",
       "  'whole',\n",
       "  'second',\n",
       "  'gram',\n",
       "  'for',\n",
       "  'lt',\n",
       "  'gt'],\n",
       " ['fyi',\n",
       "  'ha',\n",
       "  'ride',\n",
       "  'early',\n",
       "  'tomorrow',\n",
       "  'morning',\n",
       "  'but',\n",
       "  'he',\n",
       "  'crashing',\n",
       "  'at',\n",
       "  'our',\n",
       "  'place',\n",
       "  'tonight'],\n",
       " ['wow',\n",
       "  'never',\n",
       "  'realized',\n",
       "  'that',\n",
       "  'you',\n",
       "  'were',\n",
       "  'so',\n",
       "  'embarassed',\n",
       "  'by',\n",
       "  'your',\n",
       "  'accomodations',\n",
       "  'thought',\n",
       "  'you',\n",
       "  'liked',\n",
       "  'it',\n",
       "  'since',\n",
       "  'wa',\n",
       "  'doing',\n",
       "  'the',\n",
       "  'best',\n",
       "  'could',\n",
       "  'and',\n",
       "  'you',\n",
       "  'always',\n",
       "  'seemed',\n",
       "  'so',\n",
       "  'happy',\n",
       "  'about',\n",
       "  'the',\n",
       "  'cave',\n",
       "  'sorry',\n",
       "  'didn',\n",
       "  'and',\n",
       "  'don',\n",
       "  'have',\n",
       "  'more',\n",
       "  'to',\n",
       "  'give',\n",
       "  'sorry',\n",
       "  'offered',\n",
       "  'sorry',\n",
       "  'your',\n",
       "  'room',\n",
       "  'wa',\n",
       "  'so',\n",
       "  'embarassing'],\n",
       " ['sm',\n",
       "  'ac',\n",
       "  'sptv',\n",
       "  'the',\n",
       "  'new',\n",
       "  'jersey',\n",
       "  'devil',\n",
       "  'and',\n",
       "  'the',\n",
       "  'detroit',\n",
       "  'red',\n",
       "  'wing',\n",
       "  'play',\n",
       "  'ice',\n",
       "  'hockey',\n",
       "  'correct',\n",
       "  'or',\n",
       "  'incorrect',\n",
       "  'end',\n",
       "  'reply',\n",
       "  'end',\n",
       "  'sptv'],\n",
       " ['do',\n",
       "  'you',\n",
       "  'know',\n",
       "  'what',\n",
       "  'mallika',\n",
       "  'sherawat',\n",
       "  'did',\n",
       "  'yesterday',\n",
       "  'find',\n",
       "  'out',\n",
       "  'now',\n",
       "  'lt',\n",
       "  'url',\n",
       "  'gt'],\n",
       " ['congrats',\n",
       "  'year',\n",
       "  'special',\n",
       "  'cinema',\n",
       "  'pas',\n",
       "  'for',\n",
       "  'is',\n",
       "  'yours',\n",
       "  'call',\n",
       "  'now',\n",
       "  'suprman',\n",
       "  'matrix',\n",
       "  'starwars',\n",
       "  'etc',\n",
       "  'all',\n",
       "  'free',\n",
       "  'bx',\n",
       "  'ip',\n",
       "  'we',\n",
       "  'pm',\n",
       "  'dont',\n",
       "  'miss',\n",
       "  'out'],\n",
       " ['sorry', 'll', 'call', 'later', 'in', 'meeting'],\n",
       " ['tell', 'where', 'you', 'reached'],\n",
       " ['yes', 'gauti', 'and', 'sehwag', 'out', 'of', 'odi', 'series']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8ad795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "886463d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets train word2vec from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd4fd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bb22b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'to',\n",
       " 'the',\n",
       " 'your',\n",
       " 'and',\n",
       " 'for',\n",
       " 'that',\n",
       " 'in',\n",
       " 'go',\n",
       " 'call',\n",
       " 'not',\n",
       " 'free',\n",
       " 'he',\n",
       " 'like',\n",
       " 'my',\n",
       " 'is',\n",
       " 'with',\n",
       " 'have',\n",
       " 'did',\n",
       " 'all',\n",
       " 'it',\n",
       " 'or',\n",
       " 'txt',\n",
       " 'so',\n",
       " 'no',\n",
       " 'me',\n",
       " 'do',\n",
       " 'know',\n",
       " 'if',\n",
       " 'how',\n",
       " 'been',\n",
       " 'now',\n",
       " 'at',\n",
       " 'about',\n",
       " 'on',\n",
       " 'mobile',\n",
       " 'wa',\n",
       " 'anything']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get all the vocabulary\n",
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c654459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d434b7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38e320fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('at', 0.14668814837932587),\n",
       " ('not', 0.1452500820159912),\n",
       " ('you', 0.09852369129657745),\n",
       " ('on', 0.09167248755693436),\n",
       " ('or', 0.09009508043527603),\n",
       " ('no', 0.08799311518669128),\n",
       " ('do', 0.07903679460287094),\n",
       " ('in', 0.07666514813899994),\n",
       " ('now', 0.07027751207351685),\n",
       " ('about', 0.05136902630329132)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('mobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3488eef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['mobile'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0792781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'only',\n",
       " 'in',\n",
       " 'bugis',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]  # for all the words it will 100 dimension vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21c649fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(doc):\n",
    "  # remove out of vocab words\n",
    "  # sent = [word of word in doc if word in model.wv.index_to_key]\n",
    "  # print(sent)\n",
    "  \n",
    "  return np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd1d5260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\avira\\anaconda3\\envs\\condaenv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\avira\\anaconda3\\envs\\condaenv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ffbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d348843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avira\\anaconda3\\envs\\condaenv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\avira\\anaconda3\\envs\\condaenv\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 60/60 [00:00<00:00, 282.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply for the entire sentences\n",
    "\n",
    "X=[]\n",
    "for i in tqdm(range(len(words))):\n",
    "  X.append(avg_word2vec(words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "143c435c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fcf3c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent features\n",
    "X_new = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61a1e877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66cd56be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3459608605189715e-06"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7511759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependent features\n",
    "y = pd.get_dummies(messages['label'])\n",
    "y = y.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28b532d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02a5d541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eabb9026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356a2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
